{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:10<00:00,  8.20it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from model_utils import load_model, load_baseline, generate, translate_all\n",
    "from data_utils import get_bbt_data\n",
    "\n",
    "src_texts, tar_texts, prefixed = get_bbt_data(prefix=\">>nob<<\")\n",
    "\n",
    "baseline_model, baseline_tokenizer = load_baseline()\n",
    "baseline_translations = translate_all(src_texts, baseline_model, baseline_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator import Evaluator\n",
    "\n",
    "src_texts, tar_texts, prefixed = get_bbt_data(prefix=\">>nob<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/tollefj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/tollefj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/tollefj/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression ratio: 0.5\n",
      "results_csv/comp_0.5_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:05<00:00, 14.61it/s]\n",
      "84it [00:25,  3.35it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817d63205fa74ca2bbd1c7c1a3f49440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression ratio: 0.6\n",
      "results_csv/comp_0.6_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:06<00:00, 12.11it/s]\n",
      "84it [00:24,  3.47it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a8f24eaf4e4f9d93c00ddeb13cdecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression ratio: 0.7\n",
      "results_csv/comp_0.7_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:07<00:00, 11.25it/s]\n",
      "84it [00:24,  3.45it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef63084e6f7404c80426b544a2255f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression ratio: 0.8\n",
      "results_csv/comp_0.8_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:08<00:00, 10.38it/s]\n",
      "84it [00:23,  3.52it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6364da0df2b34537997501c52973ad86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression ratio: 0.9\n",
      "results_csv/comp_0.9_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:08<00:00,  9.40it/s]\n",
      "84it [00:23,  3.50it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f61306003c14de6be5bd68d7a2590d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression ratio: 1.0\n",
      "results_csv/comp_1.0_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:09<00:00,  9.27it/s]\n",
      "84it [00:23,  3.51it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5836528ae64a4d8749d7f950e7cb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "evaluator = Evaluator()\n",
    "lang = \"no\"\n",
    "\n",
    "for compression_ratio in [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "    print(f\"Compression ratio: {compression_ratio}\")\n",
    "    filename = f\"BBT/comp_{compression_ratio}_results.csv\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    print(filename)\n",
    "    with open(filename, \"w\", newline=\"\") as f:\n",
    "        model, tokenizer = load_model(compression_ratio)\n",
    "        translated = translate_all(src_texts, model, tokenizer)\n",
    "\n",
    "        csvdata = evaluator.get_csv_data(\n",
    "            predictions=translated,\n",
    "            references=tar_texts,\n",
    "            source=src_texts,\n",
    "            baselines=baseline_translations,\n",
    "            lang=lang,\n",
    "        )\n",
    "\n",
    "        writer = csv.writer(f)\n",
    "        for data in tqdm(csvdata):\n",
    "            writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "eval_path = \"BBT\"\n",
    "search = lambda x: re.search(\"comp_(.*)_results.csv\", x).group(1)\n",
    "decimals = 2\n",
    "\n",
    "translations = {}\n",
    "metrics = {}\n",
    "source = []\n",
    "target = []\n",
    "baseline = []\n",
    "baseline_metrics = []\n",
    "for file in sorted(os.listdir(eval_path), key=search):\n",
    "    if file.endswith(\".csv\"):\n",
    "        compression = search(file)\n",
    "        df = pd.read_csv(os.path.join(eval_path, file))\n",
    "        translations[compression] = df[\"Translation\"].tolist()\n",
    "        metrics[compression] = df[\"Metrics\"].tolist()\n",
    "        if len(baseline) == 0:\n",
    "            source = df[\"Source Text\"].tolist()\n",
    "            target = df[\"Original Target\"].tolist()\n",
    "            baseline = df[\"Baseline\"].tolist()\n",
    "            baseline_metrics = df[\"Baseline Metrics\"].tolist()\n",
    "\n",
    "with open(\"BBT/metric_summary.txt\", \"w\") as f:\n",
    "    for i in range(len(baseline)):\n",
    "        s = f\"Source: {source[i]}\"\n",
    "        s += f\"\\nTarget: {target[i]}\"\n",
    "        s += \"\\n\" + \"- - \"*25\n",
    "        for comp_rate in translations.keys():\n",
    "            s += f\"\\n- {comp_rate}: {translations[comp_rate][i]}\"\n",
    "            _metrics = eval(metrics[comp_rate][i])\n",
    "            # _bleu = _metrics[\"bleu\"]\n",
    "            # _r2 = _metrics[\"r2\"]\n",
    "            # _meteor = _metrics[\"meteor\"]\n",
    "            # _bert = _metrics[\"bertscore\"]\n",
    "            # metric_str = f\"BLEU: {round(_bleu, decimals)}, R2: {round(_r2, decimals)}, METEOR: {round(_meteor, decimals)}, BERTSCORE: {round(_bert, decimals)}\"\n",
    "            # s += f\"\\nMetrics {comp_rate}: {metric_str}\"\n",
    "            s += f\"\\nMetrics:\\n{_metrics}\"\n",
    "        s += \"\\n\" + \"- - \"*25\n",
    "        s += f\"\\nBaseline: {baseline[i]}\"\n",
    "        _baseline_metrics = eval(baseline_metrics[i])\n",
    "        # _baseline_metric_str = f\"BLEU: {round(_baseline_metrics['bleu'], decimals)}, R2: {round(_baseline_metrics['r2'], decimals)}, METEOR: {round(_baseline_metrics['meteor'], decimals)}, BERTSCORE: {round(_baseline_metrics['bertscore'], decimals)}\"\n",
    "        # s += f\"\\nBaseline Metrics: {_baseline_metric_str}\"\n",
    "        s += f\"\\nBaseline metrics:\\n{_baseline_metrics}\"\n",
    "        s += \"\\n\" + \"==\"*50\n",
    "        s += \"\\n\"\n",
    "        # print(s)\n",
    "        f.write(s)\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simcse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
